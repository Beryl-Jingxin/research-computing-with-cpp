---
title: MapReduce
---

## Exercise 4: MapReduce

### The 'Hello World' of MapReduce

In this example, we will demonstrate how the mapper and reducer can be applied on our local machines to count the number of times each word appears in a book.

### Choose a book

Find a good book on Project Gutenberg and download it:
[http://www.gutenberg.org/browse/scores/top](http://www.gutenberg.org/browse/scores/top)

``` bash
# The Picture of Dorian Gray
$ wget http://www.gutenberg.org/cache/epub/174/pg174.txt -O dorian.txt
```

``` bash
$ head dorian.txt

Title: The Picture of Dorian Gray

The artist is the creator of beautiful things.  To reveal art and
conceal the artist is art\'s aim.  The critic is he who can translate
into another manner or a new material his impression of beautiful
things.
```

### Hadoop streaming

Hadoop streaming enables MapReduce jobs to be run using any executable as the mapper and reducer:
[http://hadoop.apache.org/docs/r1.2.1/streaming.html](http://hadoop.apache.org/docs/r1.2.1/streaming.html)

The mapper and reducer are written to take line-by-line inputs from stdin and emit the output to stdout.

### Mapper

Our mapper:

- takes lines from our book
- extracts words from the line with a regular expression
- outputs a tab-separated string,value pair for each word (e.g. theword  1)

``` python
#!/usr/bin/env python
import sys
import re

def mapper(stream):
    pattern = re.compile('[a-zA-Z][a-zA-Z0-9]*')
    for line in stream:
        for word in pattern.findall(line):
            print word.lower() + '\t' + '1'

mapper(sys.stdin)
```

### Reducer

Our reducer:

- takes the word, value pairs generated by the mapper
- sums the count for each word
- outputs a word, count pair

``` python
#!/usr/bin/env python
import sys

def reducer(stream):
    mydict = {}
    line = stream.readline()
    while line:

        # Get the key/value pair
        line = line.strip()
        word, count = line.split('\t')
        count = int(count)

        # Add to the dict
        if word in mydict:
            mydict[word] += 1
        else:
            mydict[word] = 1

        # Get the next line
        line = stream.readline()

    # Print the aggregated key/value pairs
    # for word in sorted(mydict.keys()): # order by word
    for word in sorted(mydict, key=mydict.get, reverse=True): # or by count
        print word, '\t', mydict[word]

reducer(sys.stdin)
```

### Demonstrate locally

Download the mapper and reducer:

``` bash
$ git clone https://github.com/tompollard/dorian
```

Create a pipeline to process the book:

``` bash
# sort represents the Hadoop shuffle
$ cat dorian.txt | ./mapper.py | ./reducer.py | sort

the     3948
of      2298
and     2279
to      2181
a       1730
i       1694
he      1544
...
```
